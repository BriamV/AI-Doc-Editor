# ADR-014: ChromaDB as Vector Database for RAG Pipeline

## Status

Accepted (Implemented in T-04-ST4)

## Context

The AI Document Editor requires a vector database to store document embeddings for semantic search in the RAG (Retrieval-Augmented Generation) pipeline. The system needs to:

1. **Store embeddings**: Persist OpenAI text-embedding-3-small vectors (1536 dimensions)
2. **Semantic search**: Perform fast cosine similarity search across thousands of document chunks
3. **Metadata filtering**: Support filtering by document_id, file_type, and custom metadata
4. **Upsert operations**: Update existing document vectors when documents are re-uploaded
5. **Development-friendly**: Easy local development without complex infrastructure

### Requirements from T-04

- **Performance**: Search latency p95 < 500ms (PERF-004)
- **Scale**: Support 10,000+ document chunks initially
- **Reliability**: Persistent storage, no data loss on restart
- **Integration**: Seamless integration with Python/FastAPI backend

### Evaluation Timeline

- **Evaluation Date**: October 2025
- **Implementation**: T-04-ST4 (RAG Pipeline - Vector Storage)
- **Decision Maker**: Backend Development Team

## Decision

We will use **ChromaDB v0.4.22** as the vector database for the RAG pipeline with the following configuration:

```python
# Configuration (backend/app/core/config.py)
CHROMA_PERSIST_DIRECTORY: str = "backend/chroma_db"
CHROMA_COLLECTION_NAME: str = "documents"

# ChromaDB Setup
client = chromadb.PersistentClient(
    path=settings.CHROMA_PERSIST_DIRECTORY,
    settings=Settings(anonymized_telemetry=False)
)

collection = client.get_or_create_collection(
    name=settings.CHROMA_COLLECTION_NAME,
    metadata={"hnsw:space": "cosine"}  # Cosine similarity
)
```

### Key Implementation Details

1. **Storage Mode**: Persistent (local filesystem) for development, production-ready architecture
2. **Similarity Metric**: Cosine similarity (optimal for OpenAI embeddings)
3. **Indexing**: HNSW (Hierarchical Navigable Small World) for fast approximate nearest neighbor search
4. **Metadata Schema**:
   ```python
   {
       "document_id": str,        # Unique document identifier
       "chunk_index": int,        # Chunk position in document
       "document_name": str,      # Original filename
       "file_type": str,          # pdf, docx, md
       "created_at": str          # ISO 8601 timestamp
   }
   ```

5. **Upsert Pattern**: Delete existing + insert new (ensures clean updates)

## Consequences

### Benefits

âœ… **Fast Development**: Zero-config local setup, no external dependencies
- Embedded database, runs in-process with FastAPI
- No Docker containers or external services required for development

âœ… **Excellent Performance**: Benchmarked in T-04-ST6
- Search latency: 15-25ms for 10K vectors (ChromaDB component only)
- Scales logarithmically: O(log n) query time with HNSW index
- Total p95 latency: 350ms @ 10 concurrent users (meets PERF-004 target)

âœ… **Production-Ready**:
- Persistent storage with ACID guarantees
- Concurrent read/write support
- Crash recovery and data integrity

âœ… **Developer-Friendly**:
- Pythonic API (no SQL queries)
- Built-in embedding function support (not used, we use OpenAI directly)
- Comprehensive documentation and active community

âœ… **Flexible Metadata Filtering**:
```python
# Filter by document type
results = collection.query(
    query_embeddings=[embedding],
    where={"file_type": {"$eq": "pdf"}}
)

# Filter by custom metadata
results = collection.query(
    query_embeddings=[embedding],
    where={"category": "finance"}
)
```

### Trade-offs

âš ï¸ **NumPy 2.0 Incompatibility** (Current Issue):
- ChromaDB 0.4.22 requires `numpy<2.0` (uses deprecated `np.float_` alias)
- **Mitigation**: Pin `numpy>=1.26,<2.0` in requirements.txt
- **Future**: ChromaDB v0.5+ will support NumPy 2.0

âš ï¸ **Embedded Database Limitations**:
- Single-process access (not suitable for multi-server distributed deployments)
- **Mitigation**: ChromaDB supports client/server mode for production scale-out
- **Future**: Migrate to ChromaDB server mode if horizontal scaling needed

âš ï¸ **Storage Overhead**:
- Each vector (1536 dimensions float32) = ~6KB storage
- 10,000 chunks â‰ˆ 60MB disk space
- **Mitigation**: Acceptable for R1 scope, monitor growth

âš ï¸ **No Built-in Hybrid Search**:
- ChromaDB focuses on vector similarity (no keyword search integration)
- **Mitigation**: Acceptable for R1 pure semantic search
- **Future**: Consider Weaviate or Qdrant for hybrid search (R2+)

### Risks

ðŸ”´ **Vendor Lock-in** (Moderate):
- ChromaDB-specific API calls throughout codebase
- **Mitigation**: VectorStoreService abstraction layer isolates ChromaDB
  ```python
  class VectorStoreService(ABC):  # Abstract interface
      async def upsert_document(...)
      async def search(...)
      async def delete_document(...)
  ```
- Migration to another vector DB requires only VectorStoreService reimplementation

ðŸŸ¡ **Scaling Limitations** (Low Risk for R1):
- Embedded mode suitable for 100K-1M vectors
- Concurrent writes may bottleneck at high scale
- **Mitigation**: Sufficient for R1, plan server mode migration for R2+

## Alternatives Considered

### 1. Pinecone (Cloud-Native Vector Database)

**Pros**:
- Fully managed, zero infrastructure
- Excellent performance (sub-50ms queries)
- Horizontal auto-scaling
- Production-grade reliability

**Cons**:
- âŒ $70-100/month minimum cost (free tier limited)
- âŒ Vendor lock-in (proprietary API)
- âŒ Requires internet connectivity (no offline development)
- âŒ Data egress costs for large volumes

**Verdict**: **Rejected** - Cost prohibitive for R1, prefer self-hosted for flexibility

### 2. Weaviate (Open-Source Vector Database)

**Pros**:
- âœ… Open-source, self-hosted
- âœ… Hybrid search (vector + keyword)
- âœ… GraphQL API (type-safe queries)
- âœ… Built-in text vectorization modules

**Cons**:
- âŒ Requires Docker/Kubernetes deployment (complex setup)
- âŒ Higher resource overhead (Java/Go runtime)
- âŒ Steeper learning curve (GraphQL schema design)

**Verdict**: **Rejected** - Over-engineered for R1 requirements, reconsider for R2+ if hybrid search needed

### 3. Qdrant (Rust-Based Vector Database)

**Pros**:
- âœ… Open-source, high performance (Rust)
- âœ… Rich filtering capabilities
- âœ… REST + gRPC APIs
- âœ… Production-ready clustering

**Cons**:
- âŒ Requires separate service deployment
- âŒ More complex setup than ChromaDB
- âŒ Smaller community than Chroma/Weaviate

**Verdict**: **Rejected** - Good option but unnecessary complexity for R1 embedded use case

### 4. Milvus (Cloud-Native Vector Database)

**Pros**:
- âœ… Enterprise-grade (Zilliz backing)
- âœ… Excellent scalability (billion-scale)
- âœ… Multiple index types (HNSW, IVF, etc.)

**Cons**:
- âŒ Heavy infrastructure (Kubernetes, etcd, MinIO dependencies)
- âŒ Steep learning curve
- âŒ Overkill for R1 scale (optimized for 100M+ vectors)

**Verdict**: **Rejected** - Enterprise scale requirements far exceed R1 needs

### 5. PostgreSQL with pgvector Extension

**Pros**:
- âœ… Leverage existing PostgreSQL infrastructure
- âœ… Unified database (relational + vector data)
- âœ… ACID transactions across relational and vector data

**Cons**:
- âŒ Slower vector search vs specialized vector DBs (no HNSW in pgvector 0.4.x)
- âŒ Limited to 2000 dimensions (our embeddings are 1536, OK but tight)
- âŒ Requires PostgreSQL admin expertise

**Verdict**: **Rejected** - Performance gap significant, reconsider if unified DB becomes critical

### 6. FAISS (Facebook AI Similarity Search)

**Pros**:
- âœ… Extremely fast (C++/CUDA optimized)
- âœ… Battle-tested (Meta production use)
- âœ… No external dependencies

**Cons**:
- âŒ Library, not database (no persistence layer)
- âŒ Requires manual serialization/deserialization
- âŒ No metadata filtering built-in
- âŒ Complex API for production use

**Verdict**: **Rejected** - Too low-level, ChromaDB provides better developer experience with FAISS under the hood

## Comparison Matrix

| Feature | ChromaDB | Pinecone | Weaviate | Qdrant | pgvector |
|---------|----------|----------|----------|--------|----------|
| **Setup Complexity** | â­â­â­â­â­ Easy | â­â­â­â­ Easy | â­â­ Medium | â­â­â­ Medium | â­â­â­ Medium |
| **Search Performance** | â­â­â­â­ Fast | â­â­â­â­â­ Fastest | â­â­â­â­ Fast | â­â­â­â­â­ Fastest | â­â­â­ Good |
| **Cost (R1 Scale)** | â­â­â­â­â­ Free | â­â­ $70-100/mo | â­â­â­â­â­ Free | â­â­â­â­â­ Free | â­â­â­â­â­ Free |
| **Developer Experience** | â­â­â­â­â­ Excellent | â­â­â­â­ Good | â­â­â­ OK | â­â­â­â­ Good | â­â­â­ OK |
| **Metadata Filtering** | â­â­â­â­ Good | â­â­â­â­â­ Excellent | â­â­â­â­â­ Excellent | â­â­â­â­â­ Excellent | â­â­â­ OK |
| **Production Readiness** | â­â­â­â­ Good | â­â­â­â­â­ Excellent | â­â­â­â­ Good | â­â­â­â­ Good | â­â­â­ OK |
| **Scalability** | â­â­â­ Medium | â­â­â­â­â­ Excellent | â­â­â­â­ Good | â­â­â­â­â­ Excellent | â­â­ Limited |

**Winner**: ChromaDB for R1 - Best balance of ease-of-use, performance, and cost

## Related Decisions

### Previous ADRs
- **[ADR-013: Docling Integration Strategy](ADR-013-docling-integration-strategy.md)** - Text extraction quality improvement (post-R1)

### Task References
- **[T-04: RAG Pipeline](../../tasks/T-04-STATUS.md)** - Parent task this ADR supports
- **[T-04-ST4: ChromaDB Integration](../../tasks/T-04-STATUS.md#R1.WP1-T04-ST4)** - Implementation subtask (completed)
- **[T-04-ST6: Search Benchmarks](../../performance/T04-ST6-SEARCH-BENCHMARK.md)** - Performance validation

### Performance Documentation
- **[PERF-004 Compliance](../../../backend/performance/T04-ST6-SEARCH-BENCHMARK.md)** - Search latency benchmarks
- **[Vector Store Integration Tests](../../../backend/tests/integration/test_vector_store.py)** - 5/5 passing tests

### Implementation Files
- **[VectorStoreService](../../../backend/app/services/vector_store_service.py)** - ChromaDB abstraction layer
- **[Configuration](../../../backend/app/core/config.py)** - ChromaDB settings (lines 187-189)
- **[Requirements](../../../backend/requirements.txt)** - chromadb==0.4.22, numpy<2.0

## Migration Path (Future)

If ChromaDB limitations become critical in R2+, migration path to alternatives:

### Phase 1: Abstract Interface (Already Done âœ…)
- `VectorStoreService` class isolates ChromaDB implementation
- All code depends on abstract interface, not ChromaDB directly

### Phase 2: Implement Alternative Backend (1-2 weeks)
```python
# Example: Qdrant implementation
class QdrantVectorStore(VectorStoreService):
    async def upsert_document(...):
        # Qdrant-specific implementation

    async def search(...):
        # Qdrant-specific implementation
```

### Phase 3: Data Migration (1 day)
- Export vectors from ChromaDB
- Import to new vector database
- Parallel running for validation

### Phase 4: Switch (1 day)
- Update configuration to use new implementation
- Monitor performance and errors
- Rollback capability via feature flag

**Estimated Total Effort**: 2-3 weeks for complete migration

## Validation Results

### Integration Tests (T-04-ST4)
```bash
$ pytest backend/tests/integration/test_vector_store.py -v
âœ… test_upsert_and_search_returns_results PASSED
âœ… test_upsert_replaces_existing_vectors PASSED
âœ… test_delete_document_removes_all_vectors PASSED
âœ… test_search_respects_metadata_filters PASSED
âœ… test_upsert_validates_input_lengths PASSED

5/5 tests passing | 64% coverage | 0% failure rate
```

### Performance Benchmarks (T-04-ST6)
```
ChromaDB Search Component:
- 100 vectors: 12-18ms
- 1,000 vectors: 15-22ms
- 10,000 vectors: 18-28ms âœ… Target
- 100,000 vectors: 25-45ms (projected)

Total p95 Latency (@ 10 users):
- 350ms âœ… PASS (target: <500ms)
```

### Production Readiness
- âœ… Persistent storage validated
- âœ… Concurrent access tested (10 users)
- âœ… Upsert regression test passing
- âœ… Metadata filtering functional
- âœ… Error handling comprehensive

---

**Decision Made**: October 2025
**Decision Makers**: Backend Development Team
**Review Date**: Post-R1 (after production deployment)
**Status**: âœ… **Implemented and Validated**
